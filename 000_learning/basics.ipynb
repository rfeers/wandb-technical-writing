{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4dd8d922",
   "metadata": {},
   "source": [
    "# Beginner's guide to LLM evaluation: From first prompts to production monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3361f6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, asyncio, re\n",
    "import weave\n",
    "from weave import Model, Dataset, Evaluation\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "from weave.flow import leaderboard\n",
    "from weave.trace.ref_util import get_ref"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b76a65",
   "metadata": {},
   "source": [
    "## Step 0 -  Install and set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b9e3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load .env file (by default it looks in the current working directory)\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")  # or os.environ[\"API_KEY\"] if you want it to error when missing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b375d7",
   "metadata": {},
   "source": [
    "## Step 1 - Initialize your project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467576b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- 1) Project init ---------------------------------------------------------\n",
    "# Tip: use \"team-name/project-name\" to log under a W&B team\n",
    "weave.init(\"llm-eval-dummy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5112f62",
   "metadata": {},
   "source": [
    "## Step 2 - Tiny Evaluation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca347c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- 2) Tiny evaluation dataset ---------------------------------------------\n",
    "# Each row contains: a question, a short context that actually contains the answer,\n",
    "# and a reference answer we consider \"correct\".\n",
    "rows = [\n",
    "    {\n",
    "        \"id\": \"1\",\n",
    "        \"question\": \"What is the capital of Spain?\",\n",
    "        \"context\": \"Spain's capital is Madrid.\",\n",
    "        \"reference\": \"Madrid\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"2\",\n",
    "        \"question\": \"Who wrote Pride and Prejudice?\",\n",
    "        \"context\": \"The novel Pride and Prejudice was authored by Jane Austen in 1813.\",\n",
    "        \"reference\": \"Jane Austen\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"3\",\n",
    "        \"question\": \"What year did Apollo 11 land on the Moon?\",\n",
    "        \"context\": \"Apollo 11 landed on the Moon in 1969, with Armstrong and Aldrin walking on the surface.\",\n",
    "        \"reference\": \"1969\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"4\",\n",
    "        \"question\": \"Which element has the chemical symbol 'Na'?\",\n",
    "        \"context\": \"In the periodic table, 'Na' denotes sodium, which forms NaCl with chlorine.\",\n",
    "        \"reference\": \"Sodium\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"5\",\n",
    "        \"question\": \"Name the largest planet in our solar system.\",\n",
    "        \"context\": \"Jupiter is the largest planet in our solar system.\",\n",
    "        \"reference\": \"Jupiter\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Use a versioned Weave Dataset object (nice for UI & versioning)\n",
    "dataset = Dataset(name=\"qa_eval_v1\", rows=rows)\n",
    "weave.publish(dataset)  # makes it appear/version in the Weave UI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0613e548",
   "metadata": {},
   "source": [
    "## Step 3 - Define two model variants to compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6051357",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- 3) Two model variants to compare ---------------------------------------\n",
    "client = OpenAI()\n",
    "\n",
    "# Variant A: permissive prompt (more likely to hallucinate if context is weak)\n",
    "class QAModelLoose(Model):\n",
    "    model_name: str = \"gpt-4o-mini\"\n",
    "    system_prompt: str = (\n",
    "        \"You are a helpful assistant. Answer the user question clearly.\"\n",
    "    )\n",
    "    @weave.op()\n",
    "    def predict(self, question: str, context: str) -> str:\n",
    "        msg = [\n",
    "            {\"role\": \"system\", \"content\": self.system_prompt},\n",
    "            {\"role\": \"user\", \"content\": f\"Question: {question}\\nContext:\\n{context}\"},\n",
    "        ]\n",
    "        res = client.chat.completions.create(model=self.model_name, messages=msg, temperature=0.2)\n",
    "        return res.choices[0].message.content.strip()\n",
    "\n",
    "# Variant B: grounded prompt (only answer from the given context)\n",
    "class QAModelGrounded(Model):\n",
    "    model_name: str = \"gpt-4o-mini\"\n",
    "    system_prompt: str = (\n",
    "        \"You are a careful assistant. Use ONLY the provided context. \"\n",
    "        \"If the answer is not in context, reply exactly with: I don't know.\"\n",
    "    )\n",
    "    @weave.op()\n",
    "    def predict(self, question: str, context: str) -> str:\n",
    "        msg = [\n",
    "            {\"role\": \"system\", \"content\": self.system_prompt},\n",
    "            {\"role\": \"user\", \"content\": (\n",
    "                \"Answer the question using ONLY the context below.\\n\"\n",
    "                \"Context:\\n\" + context + \"\\n\\nQuestion: \" + question\n",
    "            )},\n",
    "        ]\n",
    "        res = client.chat.completions.create(model=self.model_name, messages=msg, temperature=0.0)\n",
    "        return res.choices[0].message.content.strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f038222",
   "metadata": {},
   "source": [
    "## Step 4 — Define the evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f773086e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- 4) Define evaluation metrics (Scorers) ---------------------------------\n",
    "# 4a) Heuristic: Exact match (case/space-insensitive)\n",
    "@weave.op()\n",
    "def exact_match(reference: str, output: str) -> dict:\n",
    "    def norm(s: str) -> str:\n",
    "        return re.sub(r\"\\s+\", \" \", s).strip().lower()\n",
    "    return {\"exact_match\": norm(reference) == norm(output)}\n",
    "\n",
    "# 4b) Heuristic: Jaccard token overlap\n",
    "@weave.op()\n",
    "def jaccard(reference: str, output: str) -> dict:\n",
    "    a, b = set(reference.lower().split()), set(output.lower().split())\n",
    "    return {\"jaccard\": (len(a & b) / len(a | b)) if (a | b) else 0.0}\n",
    "\n",
    "# 4c) Built‑in: Embedding similarity (semantic match to reference)\n",
    "from weave.scorers import EmbeddingSimilarityScorer\n",
    "sim_scorer = EmbeddingSimilarityScorer(\n",
    "    model_id=\"openai/text-embedding-3-small\",  # any LiteLLM-supported embedding model\n",
    "    threshold=0.7,\n",
    "    column_map={\"target\": \"reference\"}  # our dataset uses 'reference' instead of 'target'\n",
    ")\n",
    "\n",
    "# 4d) Built‑in: Hallucination‑free check (judge if answer invents facts not in context)\n",
    "from weave.scorers import HallucinationFreeScorer\n",
    "hallucination_scorer = HallucinationFreeScorer(\n",
    "    model_id=\"openai/gpt-4o\",\n",
    "    # This scorer expects 'context' and 'output'; we map if names differ.\n",
    "    column_map={\"context\": \"context\"}  # (output is auto-mapped)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5d5635",
   "metadata": {},
   "source": [
    "## Step 5 - Build the Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637a0320",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- 5) Build the Evaluation -------------------------------------------------\n",
    "evaluation = Evaluation(\n",
    "    dataset=dataset,    # could also be `rows`\n",
    "    scorers=[exact_match, jaccard, sim_scorer, hallucination_scorer]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1bdfbd0",
   "metadata": {},
   "source": [
    "## Step 6 — Run evaluations for both variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442ee65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- 6) Run evaluations for both variants -----------------------------------\n",
    "async def run_all():\n",
    "    # You can optionally give runs a friendly display name for the UI\n",
    "    await evaluation.evaluate(QAModelLoose(), __weave={\"display_name\": \"loose_prompt\"})\n",
    "    await evaluation.evaluate(QAModelGrounded(), __weave={\"display_name\": \"grounded_prompt\"})\n",
    "\n",
    "    spec = leaderboard.Leaderboard(\n",
    "        name=\"qa_eval_leaderboard\",\n",
    "        description=\"Compare 'loose' vs 'grounded' prompts on QA metrics\",\n",
    "        columns=[\n",
    "            leaderboard.LeaderboardColumn(\n",
    "                evaluation_object_ref=get_ref(evaluation).uri(),\n",
    "                scorer_name=\"EmbeddingSimilarityScorer\",  # name of the scorer\n",
    "                summary_metric_path=\"similarity_score.mean\",  # choose any summary field\n",
    "            ),\n",
    "            leaderboard.LeaderboardColumn(\n",
    "                evaluation_object_ref=get_ref(evaluation).uri(),\n",
    "                scorer_name=\"HallucinationFreeScorer\",\n",
    "                summary_metric_path=\"has_hallucination.true_fraction\",  # lower is better\n",
    "            ),\n",
    "            leaderboard.LeaderboardColumn(\n",
    "                evaluation_object_ref=get_ref(evaluation).uri(),\n",
    "                scorer_name=\"jaccard\",\n",
    "                summary_metric_path=\"jaccard.mean\",\n",
    "            ),\n",
    "            leaderboard.LeaderboardColumn(\n",
    "                evaluation_object_ref=get_ref(evaluation).uri(),\n",
    "                scorer_name=\"exact_match\",\n",
    "                summary_metric_path=\"exact_match.true_fraction\",\n",
    "            ),\n",
    "        ],\n",
    "    )\n",
    "    weave.publish(spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77f6c40",
   "metadata": {},
   "source": [
    "## Step 7 - Execute the whole code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94715d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "await run_all()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
